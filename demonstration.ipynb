{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from src.cira import CiRAConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained classification and labeling model must be available on your local machine. Both models can be found on Zenodo:\n",
    "\n",
    "* Classifier: [https://zenodo.org/record/5159501#.Ytq28ITP3-g](https://zenodo.org/record/5159501#.Ytq28ITP3-g)\n",
    "* Labeler (use the model named roberta_dropout_linear_layer_multilabel.ckpt for optimal performance): [https://zenodo.org/record/5550387#.Ytq3QYTP3-g](https://zenodo.org/record/5550387#.Ytq3QYTP3-g) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# specify the location of the pre-trained classification and labeling model\n",
    "model_classification = os.environ['MODEL_CLASSIFICATION']\n",
    "model_labeling = os.environ['MODEL_LABELING']\n",
    "\n",
    "# create a CiRA converter object \n",
    "cira = CiRAConverter(classifier_causal_model_path=model_classification, converter_s2l_model_path=model_labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an exemplary sentence\n",
    "sentence: str = \"If the red button is pressed and background operations are not running then the system shuts down.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CiRA classified the sentence \"If the red button is pressed and background operations are not running then the system shuts down.\" to be *causal* with a confidence of 98.47%.\n"
     ]
    }
   ],
   "source": [
    "# classify the sentence\n",
    "causal,confidence = cira.classify(sentence)\n",
    "print(f'CiRA classified the sentence \"{sentence}\" to be {\"*\" if causal else \"*non\"}causal* with a confidence of {confidence:.2%}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels associated to the sentence: [[3> (L0) Cause1  (AND L1): [3> (L4) Variable <17] [18> (L7) Condition <28] <28], [33> (L1) Cause2  (None L2): [33> (L5) Variable <54] [63> (L8) Condition <70] [55> (L10) Negation <62] <70], [76> (L2) Effect1 : [76> (L6) Variable <86] [87> (L9) Condition <97] <97], [29> (L3) Conjunction <32], [3> (L4) Variable <17], [33> (L5) Variable <54], [76> (L6) Variable <86], [18> (L7) Condition <28], [63> (L8) Condition <70], [87> (L9) Condition <97], [55> (L10) Negation <62]]\n",
      "\n",
      "Converted cause-effect-graph: ([the red button].(is pressed) && NOT [background operations].(running)) ===> [the system].(shuts down)\n",
      "\n",
      "Minimal test suite: \n",
      "  id  | the red button    background operations    | the system\n",
      "----  ------------------  -----------------------  ----------------\n",
      "   1  | is pressed        not running              | shuts down\n",
      "   2  | not is pressed    not running              | not shuts down\n",
      "   3  | is pressed        running                  | not shuts down\n"
     ]
    }
   ],
   "source": [
    "# if the sentence is causal, process it by generating the labels, graph, and consequent test suite from the sentence\n",
    "if causal:\n",
    "    labels, graph, suite = cira.process(sentence)\n",
    "\n",
    "    print(f'Labels associated to the sentence: {labels}\\n')\n",
    "    print(f'Converted cause-effect-graph: {graph}\\n')\n",
    "    print(f'Minimal test suite: \\n{suite}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4642a1b5ec3e3c46556bd567e8d2009f9640d8d51e69638f916c92691541265b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
